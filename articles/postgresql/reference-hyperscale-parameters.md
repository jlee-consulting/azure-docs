---
title: Server parameters â€“ Hyperscale (Hyperscale (Citus) - Azure Database for PostgreSQL
description: Parameters in the Hyperscale (Citus) SQL API
author: jonels-msft
ms.author: jonels
ms.service: postgresql
ms.subservice: hyperscale-citus
ms.topic: reference
ms.date: 08/10/2020
---

# Server parameters

There are various server parameters that affect the behavior of Hyperscale
(Citus), both parameters from standard PostgreSQL, and parameters specific to
Hyperscale (Citus). To learn more about PostgreSQL configuration parameters,
you can visit the [run time
configuration](http://www.postgresql.org/docs/current/static/runtime-config.html)
section of PostgreSQL documentation.

The rest of this reference aims at discussing Hyperscale (Citus) specific
configuration parameters. These parameters can be set in the Azure portal under
**Worker node parameters** under **Settings** for a Hyperscale (Citus) server
group.

> [!NOTE]
>
> Hyperscale server groups running older versions of the Citus Engine may not
> offer all the parameters listed below.

## General configuration

### citus.use\_secondary\_nodes (enum)

Sets the policy to use when choosing nodes for SELECT queries. If it
is set to 'always', then the planner will query only nodes that are
marked as 'secondary' noderole in
[pg_dist_node](reference-hyperscale-metadata.md#worker-node-table).

The supported values for this enum are:

-   **never:** (default) All reads happen on primary nodes.
-   **always:** Reads run against secondary nodes instead, and
    insert/update statements are disabled.

### citus.cluster\_name (text)

Informs the coordinator node planner which cluster it coordinates. Once
cluster\_name is set, the planner will query worker nodes in that
cluster alone.

### citus.enable\_version\_checks (boolean)

Upgrading Hyperscale (Citus) version requires a server restart (to pick up the
new shared-library), followed by the ALTER EXTENSION UPDATE command.  The
failure to execute both steps could potentially cause errors or crashes.
Hyperscale (Citus) thus validates the version of the code and that of the
extension match, and errors out if they don\'t.

This value defaults to true, and is effective on the coordinator. In
rare cases, complex upgrade processes may require setting this parameter
to false, thus disabling the check.

### citus.log\_distributed\_deadlock\_detection (boolean)

Whether to log distributed deadlock detection-related processing in the
server log. It defaults to false.

### citus.distributed\_deadlock\_detection\_factor (floating point)

Sets the time to wait before checking for distributed deadlocks. In particular
the time to wait will be this value multiplied by PostgreSQL\'s
[deadlock\_timeout](https://www.postgresql.org/docs/current/static/runtime-config-locks.html)
setting. The default value is `2`. A value of `-1` disables distributed
deadlock detection.

### citus.node\_connection\_timeout (integer)

The `citus.node_connection_timeout` GUC sets the maximum duration (in
milliseconds) to wait for connection establishment. Hyperscale (Citus) raises
an error if the timeout elapses before at least one worker connection is
established. This GUC affects connections from the coordinator to workers, and
workers to each other.

-   Default: five seconds
-   Minimum: 10 milliseconds
-   Maximum: one hour

```postgresql
-- set to 30 seconds
ALTER DATABASE foo
SET citus.node_connection_timeout = 30000;
```

## Query Statistics

### citus.stat\_statements\_purge\_interval (integer)

Sets the frequency at which the maintenance daemon removes records from
[citus_stat_statements](reference-hyperscale-metadata.md#query-statistics-table)
that are unmatched in `pg_stat_statements`. This configuration value sets the
time interval between purges in seconds, with a default value of 10. A value of
0 disables the purges.

```psql
SET citus.stat_statements_purge_interval TO 5;
```

This parameter is effective on the coordinator and can be changed at
runtime.

## Data Loading

### citus.multi\_shard\_commit\_protocol (enum)

Sets the commit protocol to use when performing COPY on a hash distributed
table. On each individual shard placement, the COPY is performed in a
transaction block to ensure that no data is ingested if an error occurs during
the COPY. However, there is a particular failure case in which the COPY
succeeds on all placements, but a (hardware) failure occurs before all
transactions commit. This parameter can be used to prevent data loss in that
case by choosing between the following commit protocols:

-   **2pc:** (default) The transactions in which COPY is performed on
    the shard placements are first prepared using PostgreSQL\'s
    [two-phase
    commit](http://www.postgresql.org/docs/current/static/sql-prepare-transaction.html)
    and then committed. Failed commits can be manually recovered or
    aborted using COMMIT PREPARED or ROLLBACK PREPARED, respectively.
    When using 2pc,
    [max\_prepared\_transactions](http://www.postgresql.org/docs/current/static/runtime-config-resource.html)
    should be increased on all the workers, typically to the same value
    as max\_connections.
-   **1pc:** The transactions in which COPY is performed on the shard
    placements is committed in a single round. Data may be lost if a
    commit fails after COPY succeeds on all placements (rare).

### citus.shard\_replication\_factor (integer)

Sets the replication factor for shards that is, the number of nodes on which
shards will be placed, and defaults to 1. This parameter can be set at run-time
and is effective on the coordinator. The ideal value for this parameter depends
on the size of the cluster and rate of node failure.  For example, you may want
to increase this replication factor if you run large clusters and observe node
failures on a more frequent basis.

### citus.shard\_count (integer)

Sets the shard count for hash-partitioned tables and defaults to 32.  This
value is used by the
[create_distributed_table](reference-hyperscale-functions.md#create_distributed_table)
UDF when creating hash-partitioned tables. This parameter can be set at
run-time and is effective on the coordinator.

### citus.shard\_max\_size (integer)

Sets the maximum size to which a shard will grow before it gets split
and defaults to 1 GB. When the source file\'s size (which is used for
staging) for one shard exceeds this configuration value, the database
ensures that a new shard gets created. This parameter can be set at
run-time and is effective on the coordinator.

## Planner Configuration

### citus.limit\_clause\_row\_fetch\_count (integer)

Sets the number of rows to fetch per task for limit clause optimization.
In some cases, select queries with limit clauses may need to fetch all
rows from each task to generate results. In those cases, and where an
approximation would produce meaningful results, this configuration value
sets the number of rows to fetch from each shard. Limit approximations
are disabled by default and this parameter is set to -1. This value can
be set at run-time and is effective on the coordinator.

### citus.count\_distinct\_error\_rate (floating point)

Hyperscale (Citus) can calculate count(distinct) approximates using the
postgresql-hll extension. This configuration entry sets the desired
error rate when calculating count(distinct). 0.0, which is the default,
disables approximations for count(distinct); and 1.0 provides no
guarantees about the accuracy of results. We recommend setting this
parameter to 0.005 for best results. This value can be set at run-time
and is effective on the coordinator.

### citus.task\_assignment\_policy (enum)

> [!NOTE]
> This GUC is applicable only when
> [shard_replication_factor](reference-hyperscale-parameters.md#citusshard_replication_factor-integer)
> is greater than one, or for queries against
> [reference_tables](concepts-hyperscale-distributed-data.md#type-2-reference-tables).

Sets the policy to use when assigning tasks to workers. The coordinator
assigns tasks to workers based on shard locations. This configuration
value specifies the policy to use when making these assignments.
Currently, there are three possible task assignment policies that can
be used.

-   **greedy:** The greedy policy is the default and aims to evenly
    distribute tasks across workers.
-   **round-robin:** The round-robin policy assigns tasks to workers in
    a round-robin fashion alternating between different replicas. This policy
    enables better cluster utilization when the shard count for a
    table is low compared to the number of workers.
-   **first-replica:** The first-replica policy assigns tasks on the
    basis of the insertion order of placements (replicas) for the
    shards. In other words, the fragment query for a shard is assigned to the worker that has the first replica of that shard.
    This method allows you to have strong guarantees about which shards
    will be used on which nodes (that is, stronger memory residency
    guarantees).

This parameter can be set at run-time and is effective on the
coordinator.

## Intermediate Data Transfer

### citus.binary\_worker\_copy\_format (boolean)

Use the binary copy format to transfer intermediate data between workers.
During large table joins, Hyperscale (Citus) may have to dynamically
repartition and shuffle data between different workers. By default, this data
is transferred in text format. Enabling this parameter instructs the database
to use PostgreSQL's binary serialization format to transfer this data. This
parameter is effective on the workers and needs to be changed in the
postgresql.conf file. After editing the config file, users can send a SIGHUP
signal or restart the server for this change to take effect.

### citus.binary\_master\_copy\_format (boolean)

Use the binary copy format to transfer data between coordinator and the
workers. When running distributed queries, the workers transfer their
intermediate results to the coordinator for final aggregation. By default, this
data is transferred in text format. Enabling this parameter instructs the
database to use PostgreSQL's binary serialization format to transfer this data.
This parameter can be set at runtime and is effective on the coordinator.

### citus.max\_intermediate\_result\_size (integer)

The maximum size in KB of intermediate results for CTEs that are unable
to be pushed down to worker nodes for execution, and for complex
subqueries. The default is 1 GB, and a value of -1 means no limit.
Queries exceeding the limit will be canceled and produce an error
message.

## DDL

### citus.enable\_ddl\_propagation (boolean)

Specifies whether to automatically propagate DDL changes from the coordinator
to all workers. The default value is true. Because some schema changes require
an access exclusive lock on tables, and because the automatic propagation
applies to all workers sequentially, it can make a Hyperscale (Citus) cluster
temporarily less responsive. You may choose to disable this setting and
propagate changes manually.

## Executor Configuration

### General

#### citus.all\_modifications\_commutative

Hyperscale (Citus) enforces commutativity rules and acquires appropriate locks
for modify operations in order to guarantee correctness of behavior. For
example, it assumes that an INSERT statement commutes with another INSERT
statement, but not with an UPDATE or DELETE statement. Similarly, it assumes
that an UPDATE or DELETE statement does not commute with another UPDATE or
DELETE statement. This precaution means that UPDATEs and DELETEs require
Hyperscale (Citus) to acquire stronger locks.

If you have UPDATE statements that are commutative with your INSERTs or
other UPDATEs, then you can relax these commutativity assumptions by
setting this parameter to true. When this parameter is set to true, all
commands are considered commutative and claim a shared lock, which can
improve overall throughput. This parameter can be set at runtime and is
effective on the coordinator.

#### citus.remote\_task\_check\_interval (integer)

Sets the frequency at which Hyperscale (Citus) checks for statuses of jobs
managed by the task tracker executor. It defaults to 10 ms. The coordinator
assigns tasks to workers, and then regularly checks with them about each
task\'s progress. This configuration value sets the time interval between two
consequent checks. This parameter is effective on the coordinator and can be
set at runtime.

#### citus.task\_executor\_type (enum)

Hyperscale (Citus) has three executor types for running distributed SELECT
queries.  The desired executor can be selected by setting this configuration
parameter. The accepted values for this parameter are:

-   **adaptive:** The default. It is optimal for fast responses to
    queries that involve aggregations and colocated joins spanning
    across multiple shards.
-   **task-tracker:** The task-tracker executor is well suited for long
    running, complex queries that require shuffling of data across
    worker nodes and efficient resource management.
-   **real-time:** (deprecated) Serves a similar purpose as the adaptive
    executor, but is less flexible and can cause more connection
    pressure on worker nodes.

This parameter can be set at run-time and is effective on the coordinator.

#### citus.multi\_task\_query\_log\_level (enum) {#multi_task_logging}

Sets a log-level for any query that generates more than one task (that is,
which hits more than one shard). Logging is useful during a multi-tenant
application migration, as you can choose to error or warn for such queries, to
find them and add a tenant\_id filter to them. This parameter can be set at
runtime and is effective on the coordinator. The default value for this
parameter is \'off\'.

The supported values for this enum are:

-   **off:** Turn off logging any queries that generate multiple tasks
    (that is, span multiple shards)
-   **debug:** Logs statement at DEBUG severity level.
-   **log:** Logs statement at LOG severity level. The log line will
    include the SQL query that was run.
-   **notice:** Logs statement at NOTICE severity level.
-   **warning:** Logs statement at WARNING severity level.
-   **error:** Logs statement at ERROR severity level.

It may be useful to use `error` during development testing,
and a lower log-level like `log` during actual production deployment.
Choosing `log` will cause multi-task queries to appear in the database
logs with the query itself shown after \"STATEMENT.\"

```
LOG:  multi-task query about to be executed
HINT:  Queries are split to multiple tasks if they have to be split into several queries on the workers.
STATEMENT:  select * from foo;
```

#### citus.enable\_repartition\_joins (boolean)

Ordinarily, attempting to perform repartition joins with the adaptive executor
will fail with an error message.  However setting
`citus.enable_repartition_joins` to true allows Hyperscale (Citus) to
temporarily switch into the task-tracker executor to perform the join.  The
default value is false.

### Task tracker executor configuration

#### citus.task\_tracker\_delay (integer)

This parameter sets the task tracker sleep time between task management rounds
and defaults to 200 ms. The task tracker process wakes up regularly, walks over
all tasks assigned to it, and schedules and executes these tasks.  Then, the
task tracker sleeps for a time period before walking over these tasks again.
This configuration value determines the length of that sleeping period. This
parameter is effective on the workers and needs to be changed in the
postgresql.conf file. After editing the config file, users can send a SIGHUP
signal or restart the server for the change to take effect.

This parameter can be decreased to trim the delay caused due to the task
tracker executor by reducing the time gap between the management rounds.
Decreasing the delay is useful in cases when the shard queries are short and
hence update their status regularly.

#### citus.max\_assign\_task\_batch\_size (integer)

The task tracker executor on the coordinator synchronously assigns tasks in
batches to the daemon on the workers. This parameter sets the maximum number of
tasks to assign in a single batch. Choosing a larger batch size allows for
faster task assignment. However, if the number of workers is large, then it may
take longer for all workers to get tasks.  This parameter can be set at runtime
and is effective on the coordinator.

#### citus.max\_running\_tasks\_per\_node (integer)

The task tracker process schedules and executes the tasks assigned to it as
appropriate. This configuration value sets the maximum number of tasks to
execute concurrently on one node at any given time and defaults to 8.

The limit ensures that you don\'t have many tasks hitting disk at the same
time, and helps in avoiding disk I/O contention. If your queries are served
from memory or SSDs, you can increase max\_running\_tasks\_per\_node without
much concern.

#### citus.partition\_buffer\_size (integer)

Sets the buffer size to use for partition operations and defaults to 8 MB.
Hyperscale (Citus) allows for table data to be repartitioned into multiple
files when two large tables are being joined. After this partition buffer fills
up, the repartitioned data is flushed into files on disk.  This configuration
entry can be set at run-time and is effective on the workers.

### Explain output

#### citus.explain\_all\_tasks (boolean)

By default, Hyperscale (Citus) shows the output of a single, arbitrary task
when running
[EXPLAIN](http://www.postgresql.org/docs/current/static/sql-explain.html) on a
distributed query. In most cases, the explain output will be similar across
tasks. Occasionally, some of the tasks will be planned differently or have much
higher execution times. In those cases, it can be useful to enable this
parameter, after which the EXPLAIN output will include all tasks. Explaining
all tasks may cause the EXPLAIN to take longer.

## Next steps

* Another form of configuration, besides server parameters, are the resource [configuration options](concepts-hyperscale-configuration-options.md) in a Hyperscale (Citus) server group.
* The underlying PostgreSQL data base also has [configuration parameters](http://www.postgresql.org/docs/current/static/runtime-config.html).
