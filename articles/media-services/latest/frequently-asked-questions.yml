### YamlMime:FAQ
metadata:
  title: Azure Media Services FAQ
  description: "Get answers to common questions about Azure Media Services."
  ms.service: media-services
  ms.custom: contperf-fy21q4
    
title: Common questions about Azure Media Services
summary: This article answers commonly asked questions about Azure Media.
sections:
  - name: Developing with SDKs
    questions:
    - question: Where can I find the Media Services API and SDKs?
      answer: |
        Here is a listing:
        - [OpenAPI Specification (Swagger)](https://aka.ms/ams-v3-rest-sdk)
        - [Azure CLI](/cli/azure/install-azure-cli)
        - [.NET](https://aka.ms/ams-v3-dotnet-sdk)
        - [Java](https://aka.ms/ams-v3-java-sdk)
        - [Python](https://aka.ms/ams-v3-python-sdk)
        - [Node.js](https://aka.ms/ams-v3-nodejs-sdk)
        - [Go](https://aka.ms/ams-v3-go-sdk)
        - [Ruby](https://aka.ms/ams-v3-ruby-sdk)
    - question: Should I use the client SDKs or write directly to the REST API? 
      answer: |
         It is not advised to attempt to wrap the REST API for Media Services directly into your own library code, as doing so properly for production purposes would require you to implement the full Azure Resource Management retry logic and understand how to manage long running operations in Azure Resource Management APIs. This is handled by the client SDKs for various language - .NET, Java, Typescript, Python, Ruby, etc. - for you automatically and reduces the chances of you having issues with rety logic or failed API calls. The client SDKs all handle this for you already. The Postman collection is provided more as a teaching tool, and to show you what the client SDKs are actually doing on the wire during your development with the various client SDKs.
    - question: Where can I find Media Services samples?
      answer: |
        See the article [Media Services v3 samples](samples-overview.md) for a listing of samples.
    - question: How does paging on large result sets (i.e. assets listing) work in the API?
      answer: |
        When you're using pagination, you should always use the next link to enumerate the collection and not depend on a particular page size. For details and examples, see [Filtering, ordering, paging](filter-order-page-entities-how-to.md).
  - name: Accounts
    questions:
    - question: How do I use a Managed Identity to encrypt data to Media Services? 
      answer: |
        Take a look at the [Use a Key Vault key to encrypt data into a Media Services account tutorial](security-encrypt-data-managed-identity-cli-tutorial.md) for using the CLI to pair Media Services with Key Vault to encrypt your data.  
    - question: How do I use a Managed Identity to give Media Services access to a restricted Storage account? 
      answer: |
        If you would like to access a storage account when the storage account is configured to block requests from unknown IP addresses,  follow the steps in [Access storage with a Media Services Managed Identity](security-access-storage-managed-identity-cli-tutorial.md).
    - question: What is the process of moving a Media Services account between subscriptions?  
      answer: |
        For details, see [Moving a Media Services account between subscriptions](account-move-account-how-to.md).
  - name: Security
    questions:
    - question: What Azure roles can perform actions on Azure Media Services resources?
      answer: |
        See [Azure role-based access control (Azure RBAC) for Media Services accounts](security-rbac-concept.md).

  - name: Assets, uploading and storage
    questions:
    - question: What is a Media Services Asset?
      answer: | 
        A Media Services asset is an Azure Storage account container that is used for each video file you upload.  It has a unique identifier that is used with Transforms and other operations.  See [Assets in Media Service v3](assets-concept.md).
    - question: How do I create a Media Services Asset?
      answer: | 
        Every time you want to upload a media file and perform and do something with it like encoding or streaming, you create an Asset to store the media file and other files associated with that media file. Assets are automatically created for you if you use the Azure portal. If you are not using the portal to upload files, you must create an Asset first.  See [Create an Asset](asset-create-asset-how-to.md) to learn how to create one.
  
  - name: Encoding
    questions:
    - question: What are the encoding formats available with Media Services?
      answer: |
        The common encoding formats are available with Media Services Standard Encoder. For a list of all formats, see [Standard Encoder formats and codecs](encode-media-encoder-standard-formats-reference.md).
    - question: How do I create a Media Services Job?
      answer: |
        You can create a Job in the Azure portal, with [CLI](job-create-cli-how-to.md), REST or any of the SDKs. See the [Media Services samples](https://github.com/Azure-Samples?q=media-services&type=&language=&sort=) for the language you prefer.
    - question: Can I use Media Services to create an auto-generated bitrate ladder?
      answer: |
        Yes.  You'll find more information in the [Encode with an auto-generated bitrate ladder](./encode-autogen-bitrate-ladder.md) page.
    - question: Does Media Services support content aware encoding?
      answer: |
        Yes Media Services can perform two pass analysis on a video and output the best recommended adaptive bitrate set, resolutions, and encoding settings based on contents of the video provided. 
        See details on the [content aware encoding preset](./encode-content-aware-how-to.md)
    - question: Can I use an externally encoded or existing Mp4 files in Media Services?
      answer: | 
        Yes, see the section under Streaming questions on "Can I Stream existing MP4 files" for details and links to a sample application showing how to upload a single bitrate Mp4 file that is pre-encoded and generate the server manifest (.ism) and client manifest(.ismc).
        Note the performance impact issues on the origin that are called out in that question/answer as well.
    - question: Can  Media Services be used for very short-form file content encoding?
      answer: | 
        Short answer, not recommended.  Very short content that is less than a minute or two in duration is not really ideal for Adaptive Bitrate streaming.  If you intend to stream very short form files, it is recommended to pre-encode the content into a format that is 
        easily streamed using a single bitrate. 
        
        Since most adaptive bitrate players need time to buffer multiple segments of video, as well as time to analyze the network bandwidth before "shifting" up or down the adaptive bitrate ladder, it is often useless to provide 
        a lot of bitrates for content that is under 30 seconds long.  By the time the player locks it's heuristic algorithm on the right bitrate to play due to network conditions, the file will be done streaming.  
        
        In addition, some players default to buffering up to 3 "segments" of video.  Each segment can be 2 to 6 seconds in duration.  For very short format videos, the player is likely to buffer and begin playback of the first selected bitrate of the ABR set. 
        For this reason, we recommend using a single bitrate MP4 file, and uploading it to an Asset if you require HLS or DASH manifest generation. See the above question titles "Can I stream existing Mp4 files..." for details on how to achieve this. 
        
        It's only necessary to deliver the files in HLS or DASH format if you want to benefit from the capabilities of those protocols.  For single bitrate streams, they can still offer a lot - such as faster seeking, DRM support, more difficult to download via URL (but still possible!) than a
        progressive download Mp4 in blob storage. Captions support for VTT and IMSC1 are also another benefit.  In addition, the ability to late bind additional audio renditions, or dubbings in alternate languages makes this a valuable choice for some situations. 

  - name: Live streaming
    questions:
    - question: What is a Media Services Live Event?
      answer: |
        A Media Services live event is the process of ingesting live video feeds and broadcasting them through an RTMPS protocol or Smooth Streaming. For more information about Media Services Live Events, see [Live events and live outputs in Media Services](live-event-outputs-concept.md)
    - question: How do I create a Media Services Live Event?
      answer: |
        The first steps is to choose an on premises encoder.  We have provided examples for creating a Live Event with [Wirecast](live-event-wirecast-quickstart.md) and [OBS](live-event-obs-quickstart.md). If you would rather start with an overview of Media Services Live Events, see [Live Event Types](stream-live-streaming-concept.md#live-event-types).
    - question: How do I do live transcription with a Media Services Live Event?
      answer: |
        Azure Media Service delivers video, audio, and text in different protocols. When you publish your live stream using MPEG-DASH or HLS/CMAF, then along with video and audio, our service delivers the transcribed text in IMSC1.1 compatible TTML. For more information about Live Transcription, see [Live transcription](live-event-live-transcription-how-to.md)
    - question: How do I monitor my live event's health?
      answer: |
        You can monitor live events by subscribing to Azure Event Grid events. For more information, see the [EventGrid event schema](monitoring/media-services-event-schemas.md#live-event-types).
        You can either:
        * [Subscribe](monitoring/reacting-to-media-services-events.md) to the stream-level [Microsoft.Media.LiveEventEncoderDisconnected](monitoring/media-services-event-schemas.md#liveeventencoderdisconnected) events and monitor that no reconnections come in for a while to stop and delete your live event.
        * [Subscribe](monitoring/reacting-to-media-services-events.md) to the track-level [heartbeat](monitoring/media-services-event-schemas.md#liveeventingestheartbeat) events. If all tracks have an incoming bitrate dropping to 0 or the last time stamp is no longer increasing, you can safely shut down the live event. The heartbeat events come in at every 20 seconds for every track, so it might be a bit verbose.
    - question: Can I re-use the same streaming URL on re-start of a Live Event?
      answer: | 
        No, you cannot easily use the same streaming URL if you stop and start a live event. 
        Each time you create and publish a new live output (and asset), a new streaming URL (GUID) will be used for the new locator. 
        That way, you are sure there won't be any cache conflict in the Streaming Endpoint and the CDN. 
        You can prepare (and know) the streaming URLs in advance because you can force a specific GUID for the streaming locator, and decide the manifest name to use for the live output.
        
        So let's say, you decide to use GUID 1a7ed69e-a361-433d-8a56-29c61872744f for the live output that you will create tomorrow. When this day comes, you start the live event and create a live output. You can decide to use "conference1" for the manifest and force the GUID for the locator.

        The streaming URL is predictable and is `http://<youraccountname>-<azureregion>.streaming.media.azure.net/1a7ed69e-a361-433d-8a56-29c61872744f/conference1.ism/manifest`

        You cannot reuse the same live output / asset multiple times. You should think of the combination of the Live output + asset as a tape recording. 
        Once the live has been recorded to the asset, you cannot reuse it for another recording. There will be blob conflict or overwrites if you did that again. 
        Unless you plan to full purge the blobs in the storage account, and purge the CDN completely, there will be issues. 
        There will likely still be issues because the fragments have are already cached downstream at the CDN or in customer client device caches (browser cache, etc.)


  - name: Packaging and delivery
    questions:
    - question: I uploaded, encoded, and published a video. Why won't the video play when I try to stream it?
      answer: |
        One of the most common reasons is that you don't have the streaming endpoint from which you're trying to play back in the Running state.
    - question: What is a Media Services Streaming Endpoint?
      answer: |
        In Media Services, a Streaming Endpoint represents a dynamic (just-in-time) packaging and origin service that can deliver your live and on-demand content directly to a client player app using one of the common streaming media protocols (HLS or DASH). In addition, the Streaming Endpoint provides dynamic (just-in-time) encryption to industry-leading DRMs.  For more information about Streaming Endpoints, see [Streaming Endpoints (Origin) in Azure Media Services](stream-streaming-endpoint-concept.md)
    - question: What is a Media Services Streaming Locator?
      answer: |
        To make videos in the available to clients for playback, you create a Streaming Locator and then build streaming URLs. Streaming Locators are also used to apply Streaming Policies that contain rules for how the media files are consumed.
    - question: How do I create a Media Services Streaming Locator?
      answer: |
        To build a streaming URL, you first create a Streaming Locator. You then concatenate the Streaming Endpoint host name and the Streaming Locator path.  See [Create a streaming locator and build URLs](create-streaming-locator-build-url.md).
    - question: What is a streaming policy?
      answer: |
        Streaming Policies enable you to define streaming protocols and encryption options for your Streaming Locators. Media Services v3 provides some predefined Streaming Policies. See [Streaming Policies](./stream-streaming-policy-concept.md) for more information.
    - question: How do I create a Media Services streaming policy?
      answer: | 
        See [Streaming Policies](./stream-streaming-policy-concept.md) for a list of predefined policies you can use to get started.
    - question: How do I stream HLS format content to Apple devices?
      answer: |
        Make sure you have **(format=m3u8-cmaf)** at the end of your path (after the **/manifest** portion of the URL) to tell the streaming origin server to return HTTP Live Streaming (HLS) content for consumption on Apple iOS native devices. For details, see [Delivering content](encode-dynamic-packaging-concept.md).
    - question: Can I stream existing Mp4 files that are pre-encoded or encoded in another solution?
      answer: | 
        Yes, Media Services origin server (Streaming Endpoint) supports dynamic packaging of MP4 files to HLS or DASH streaming format. However, the content must be encoded in closed-GOP format, with short GOPs in the 2-6 second duration range. 
        We recommend the following settings, 2 second GOPs,  Key frame max and min distance of 2 seconds, Constant Bitrate encoding (CBR mode). Most content in this format that is encoded using H264 or HEVC video codec along with AAC audio format can be supported. 
        Additional audio formats may also be supported that are pre-encoded, such as Dolby DD+. 
        
        The key to making it work is to create an Asset, upload the pre-encoded assets into the Asset's container using Blob Storage client SDKs, and then generate the required server manifest (.ism) and client manifest files. 
        There is an example provided in the following .NET Samples project showing how to do this.  
        See the sample project for details- [Stream existing MP4 files](https://github.com/Azure-Samples/media-services-v3-dotnet/tree/main/Streaming/StreamExistingMp4)

        Keep in mind that there are performance implications when using this approach, as the built in encoder in AMS also generates binary indexes (.mpi files) that improve the access time into the MP4 files.  Without these files, the server can use slightly more CPU at high load.eaming an existing single bitrate MP4 file with HLS or Dash](https://github.com/Azure-Samples/media-services-v3-dotnet/tree/main/Streaming/StreamExistingMp4)
        You should monitor the Streaming Endpoint CPU load in Azure Monitor when scaling up with this approach.  If you are planning to go to production with a large library of MP4 files that are pre-encoded outside of AMS, please file a support ticket to have your architecture reviewed and
        inquire about ways to improve the origin server performance of pre-encoded Mp4 content. 

  - name: Content protection
    questions:
    - question: How do I deliver my media content with dynamic encryption?
      answer: | 
        Dynamic encryption is securing your media from the time it leaves your computer all the way through storage, processing, and delivery. With Media Services, you can deliver your live and on-demand content encrypted dynamically with Advanced Encryption Standard (AES-128) or any of the three major digital rights management (DRM) systems: Microsoft PlayReady, Google Widevine, and Apple FairPlay. See [Protect your content with Media Services dynamic encryption](./drm-content-protection-concept.md).
    - question: Should I use AES-128 clear key encryption or a DRM system?
      answer: |
        Customers often wonder whether they should use AES encryption or a DRM system. The main difference between the two systems is that with AES encryption, the content key is transmitted to the client over TLS so that the key is encrypted in transit but without any additional encryption ("in the clear"). As a result, the key that's used to decrypt the content is accessible to the client player and can be viewed in a network trace on the client in plain text. AES-128 clear key encryption is suitable for use cases where the viewer is a trusted party (for example, encrypting corporate videos distributed within a company to be viewed by employees).

        DRM systems like PlayReady, Widevine, and FairPlay all provide an additional level of encryption on the key that's used to decrypt the content, compared to an AES-128 clear key. The content key is encrypted to a key protected by the DRM runtime in addition to any transport-level encryption provided by TLS. Additionally, decryption is handled in a secure environment at the operating system level, where it's more difficult for a malicious user to attack. We recommend DRM for use cases where the viewer might not be a trusted party and you need the highest level of security.
    - question: How do I show a video to only users who have a specific permission, without using Azure AD?
      answer: |
        You don't have to use any specific token provider such as Azure Active Directory (Azure AD). You can create your own [JWT](../../active-directory/develop/security-tokens.md#json-web-tokens-and-claims) provider (so-called Secure Token Service, or STS) by using asymmetric key encryption. In your custom STS, you can add claims based on your business logic.
        
        Make sure that the issuer, audience, and claims all match up exactly between what's in JWT and the `ContentKeyPolicyRestriction` value used in `ContentKeyPolicy`.

        For more information, see [Protect your content by using Media Services dynamic encryption](drm-content-protection-concept.md).
    - question: How and where did I get a JWT token before using it to request a license or key?
      answer: |
        For production, you need to have Secure Token Service (that is, a web service), which issues a JWT token upon an HTTPS request. For test, you can use the code shown in the `GetTokenAsync` method defined in [Program.cs](https://github.com/Azure-Samples/media-services-v3-dotnet-tutorials/blob/main/AMSV3Tutorials/EncryptWithDRM/Program.cs).

        The player makes a request, after a user is authenticated, to STS for such a token and assigns it as the value of the token. You can use the [Azure Media Player API](https://amp.azure.net/libs/amp/latest/docs/).

        For an example of running STS with either a symmetric key or an asymmetric key, see the [JWT tool](https://aka.ms/jwt). For an example of a player based on Azure Media Player using such a JWT token, see the [Azure media test tool](https://aka.ms/amtest). (Expand the **player_settings** link to see the token input.)
    - question: How do I authorize requests to stream videos with AES encryption?
      answer: |
        The correct approach is to use Secure Token Service. In STS, depending on the user profile, add different claims (such as "Premium User," "Basic User," "Free Trial User"). With different claims in a JWT, the user can see different contents. For different contents or assets, `ContentKeyPolicyRestriction` will have the corresponding `RequiredClaims` value.

        Use Azure Media Services APIs for configuring license/key delivery and encrypting your assets (as shown in [this sample](https://github.com/Azure-Samples/media-services-v3-dotnet-tutorials/blob/main/AMSV3Tutorials/EncryptWithAES/Program.cs)).

        For more information, see:

        - [Content protection overview](drm-content-protection-concept.md)
        - [Design of a multi-DRM content protection system with access control](architecture-design-multi-drm-system.md)
    - question: Why does only audio play but not video when using FairPlay offline mode?
      answer: |
        This behavior seems to be by design of the sample app. When an alternate audio track is present (which is the case for HLS) during offline mode, both iOS 10 and iOS 11 default to the alternate audio track. To compensate this behavior for FPS offline mode, remove the alternate audio track from the stream. To do this on Media Services, add the dynamic manifest filter **audio-only=false**. In other words, an HLS URL ends with **.ism/manifest(format=m3u8-aapl,audio-only=false)**. 
    - question: Why does FairPlay offline play audio only without video  mode I add audio-only=false?
      answer: |
        Depending on the cache key design for the content delivery network, the content might be cached. Purge the cache.
    - question: What is the downloaded/offline file structure on iOS devices?
      answer: |    
        The downloaded file structure on an iOS device looks like the following screenshot. The `_keys` folder stores downloaded FPS licenses, with one store file for each license service host. The `.movpkg` folder stores audio and video content. 

        The first folder with a name that ends with a dash followed by a number contains video content. The numeric value is the peak bandwidth of the video renditions. The second folder with a name that ends with a dash followed by 0 contains audio content. The third folder named `Data` contains the master playlist of the FPS content. Finally, boot.xml provides a complete description of the `.movpkg` folder content. 

        ![Offline file structure for the FairPlay iOS sample app](media/drm-offline-fairplay-for-ios-concept/offline-fairplay-file-structure.png)

        Here's a sample boot.xml file:
        ```xml
        <?xml version="1.0" encoding="UTF-8"?>
        <HLSMoviePackage xmlns:xsi="https://www.w3.org/2001/XMLSchema-instance" xmlns="http://apple.com/IMG/Schemas/HLSMoviePackage" xsi:schemaLocation="http://apple.com/IMG/Schemas/HLSMoviePackage /System/Library/Schemas/HLSMoviePackage.xsd">
          <Version>1.0</Version>
          <HLSMoviePackageType>PersistedStore</HLSMoviePackageType>
          <Streams>
            <Stream ID="1-4DTFY3A3VDRCNZ53YZ3RJ2NPG2AJHNBD-0" Path="1-4DTFY3A3VDRCNZ53YZ3RJ2NPG2AJHNBD-0" NetworkURL="https://willzhanmswest.streaming.mediaservices.windows.net/e7c76dbb-8e38-44b3-be8c-5c78890c4bb4/MicrosoftElite01.ism/QualityLevels(127000)/Manifest(aac_eng_2_127,format=m3u8-aapl)">
              <Complete>YES</Complete>
            </Stream>
            <Stream ID="0-HC6H5GWC5IU62P4VHE7NWNGO2SZGPKUJ-310656" Path="0-HC6H5GWC5IU62P4VHE7NWNGO2SZGPKUJ-310656" NetworkURL="https://willzhanmswest.streaming.mediaservices.windows.net/e7c76dbb-8e38-44b3-be8c-5c78890c4bb4/MicrosoftElite01.ism/QualityLevels(161000)/Manifest(video,format=m3u8-aapl)">
              <Complete>YES</Complete>
            </Stream>
          </Streams>
          <MasterPlaylist>
            <NetworkURL>https://willzhanmswest.streaming.mediaservices.windows.net/e7c76dbb-8e38-44b3-be8c-5c78890c4bb4/MicrosoftElite01.ism/manifest(format=m3u8-aapl,audio-only=false)</NetworkURL>
          </MasterPlaylist>
          <DataItems Directory="Data">
            <DataItem>
              <ID>CB50F631-8227-477A-BCEC-365BBF12BCC0</ID>
              <Category>Playlist</Category>
              <Name>master.m3u8</Name>
              <DataPath>Playlist-master.m3u8-CB50F631-8227-477A-BCEC-365BBF12BCC0.data</DataPath>
              <Role>Master</Role>
            </DataItem>
          </DataItems>
        </HLSMoviePackage>
        ```
    - question:  How can I deliver persistent licenses (offline enabled) for some clients/users and non-persistent licenses (offline disabled) for others? Do I have to duplicate the content and use separate content keys?
      answer: |
        Because Media Services v3 allows an asset to have multiple `StreamingLocator` instances, you can have:

        * One `ContentKeyPolicy` instance with `license_type = "persistent"`, `ContentKeyPolicyRestriction` with claim on `"persistent"`, and its `StreamingLocator`.
        * Another `ContentKeyPolicy` instance with `license_type="nonpersistent"`, `ContentKeyPolicyRestriction` with claim on `"nonpersistent`", and its `StreamingLocator`.
        * Two `StreamingLocator` instances that have different `ContentKey` values.

        Depending on business logic of custom STS, different claims are issued in the JWT token. With the token, only the corresponding license can be obtained and only the corresponding URL can be played.
    - question: What is the mapping between the Widevine and Media Services DRM security levels?
      answer: |
        Google's "Widevine DRM Architecture Overview" defines three security levels. However, the [Azure Media Services documentation on the Widevine license template](drm-widevine-license-template-concept.md) outlines
        five security levels (client robustness requirements for playback). This section explains how the security levels map.

        Both sets of security levels are defined by Google Widevine. The difference is in usage level: architecture or API. The five security levels are used in the Widevine API. The `content_key_specs` object, which
        contains `security_level`, is deserialized and passed to the Widevine global delivery service by the Azure Media Services Widevine license service. The following table shows the mapping between the two sets of security levels.

        | **Security levels defined in Widevine architecture** |**Security levels used in Widevine API**|
        |---|---| 
        | **Security Level 1**: All content processing, cryptography, and control are performed within the Trusted Execution Environment (TEE). In some implementation models, security processing might be performed in different chips.|**security_level=5**: The crypto, decoding, and all handling of the media (compressed and uncompressed) must be handled within a hardware-backed TEE.<br/><br/>**security_level=4**: The crypto and decoding of content must be performed within a hardware-backed TEE.|
        **Security Level 2**: Cryptography (but not video processing) is performed within the TEE. Decrypted buffers are returned to the application domain and processed through separate video hardware or software. At Level 2, however, cryptographic information is still processed only within the TEE.| **security_level=3**: The key material and crypto operations must be performed within a hardware-backed TEE. |
        | **Security Level 3**: There's no TEE on the device. Appropriate measures can be taken to protect the cryptographic information and decrypted content on host operating system. A Level 3 implementation might also include a hardware cryptographic engine, but that enhances only performance, not security. | **security_level=2**: Software crypto and an obfuscated decoder are required.<br/><br/>**security_level=1**: Software-based white-box crypto is required.|

  - name: Monitoring
    questions:
    - question: How do I monitor my Media Services resources?
      answer: |
        Use Azure Monitor to keep track of what is happening with your Media Services resources.  For more information, see [Monitor Media Services](./monitoring/monitor-media-services.md). How to guides include [Monitor Media Services metrics](./monitoring/media-services-metrics-howto.md) and [Monitor Media Services diagnostic logs](./monitoring/media-services-diagnostic-logs-howto.md).
    - question: How do I monitor my Media Services Live Event? 
      answer: |
        Use [Azure Event Grid](./monitoring/reacting-to-media-services-events.md) to monitor your live event without a polling service. How to guides include [Create and monitor Media Services events with Event Grid using the Azure portal](./monitoring/monitor-events-portal-how-to.md) and [Create and monitor Media Services events with Event Grid using the Azure CLI](./monitoring/job-state-events-cli-how-to.md).

  - name: Players
    questions:
    - question: Which video players can I use with Media Services?  
      answer: |
        Media Services works with Azure Media Player, Shaka, and Video.js. See the [Azure Media Player documentation](../azure-media-player/azure-media-player-overview.md), [How to use the Shaka player with Azure Media Services](./player-shaka-player-how-to.md), or [How to use the Video.js player with Azure Media Services](./player-media-players-concept.md).
    
  - name: High Availability
    questions:
    - question: Does Media Services support high availability?
      answer: |
        More information about Media Services and high availability is located in [High Availability with Media Services and Video on Demand (VOD)](./architecture-high-availability-encoding-concept.md).
   
  - name: Migrating from v2
    questions:
    - question: How do I migrate from Media Services v2 to Media Services v3? 
      answer: |
        We have created a [comprehensive guide for migration from v3 to v3](./migrate-v-2-v-3-migration-introduction.md).  We are very interested in knowing about your migration experience and needs so please feel free to provide feedback via GitHub issue or support ticket.
  
  - name: Troubleshooting
    questions:
    - question: How do I find out what this error code means?
      answer: |
        We have documented error codes in the following references: [streaming endpoint error codes](./stream-streaming-endpoint-error-codes-reference.md), [live event error codes](./live-event-error-codes-reference.md), [job error codes](./job-error-codes-reference.md).  If you can't find answers there, please create a support ticket.
    - question: How do I reset my account credentials?
      answer: |
        You can [reset your account credentials](./account-reset-account-credentials.md) with the CLI.

  - name: Billing and cost estimates
    questions:
    - question: How much does Media Services cost?
      answer: |
        See the [Media Services Pricing Guide](https://azure.microsoft.com/pricing/details/media-services/)

  - name: Quotas and limitations
    questions:
    - question: What quotas and limits are there for Media Services?
      answer: |
        See [Media Services quotas and limits](limits-quotas-constraints-reference.md)

  - name: Compliance and Customer data
    questions:
      - question: Does Media Services store any customer data outside of the service region?
        answer: | 
          - Customers attach their own storage accounts to their Azure Media Services account.  All asset data is stored in these associated storage accounts and the customer controls the location and replication type of this storage.
          - Additional data associated with the Media Services account (including Content Encryption Keys, token verification keys, JobInputHttp urls, and other entity metadata) is stored in Microsoft owned storage within the region selected for the Media Services account.
            - Due to [data residency requirements](https://azure.microsoft.com/global-infrastructure/data-residency/#more-information) in Brazil South and Southeast Asia, the additional account data is stored in a zone-redundant fashion and is contained in a single region. For Southeast Asia, all the additional account data is stored in Singapore and for Brazil South, the data is stored in Brazil.
            - In regions other than Brazil South and Southeast Asia, the additional account data may also be stored in Microsoft owned storage in the [paired region](../../best-practices-availability-paired-regions.md).
          - Azure Media Services is a regional service and does not provide [high availability](architecture-high-availability-encoding-concept.md) or data replication. Customers needing these features are highly encouraged to build a solution using Media Services accounts in multiple regions.  A sample showing how to build a solution for High Availability with Media Services Video on Demand is available as a guide.
      - question: Does Media Services provide high availability or data replication? 
        answer: |
          -	Azure Media Services is a regional service and does not provide high availability or data replication. Customers needing these features are highly encouraged to build a solution using Media Services accounts in multiple regions. A sample showing how to build a solution for [High Availability with Media Services Video on Demand](./architecture-high-availability-encoding-concept.md) is available as a guide.
